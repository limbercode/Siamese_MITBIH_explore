{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptim\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m lr_scheduler\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptim\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01moptim\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import h5py as hp\n",
    "from trainer import fit\n",
    "import numpy as np\n",
    "from torch.utils import data \n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "ECG_class=['N','V','L','R']\n",
    "\n",
    "def load_mat(path_data,name_data,dtype='float32'):\n",
    "    data=hp.File(path_data)\n",
    "    arrays_d={}\n",
    "    for k,v in data.items():\n",
    "        arrays_d[k]=np.array(v)\n",
    "    dataArr=np.array(arrays_d[name_data],dtype=dtype)\n",
    "    return dataArr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data and divide training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path='./' #自定义路径要正确\n",
    "DataFile='Data_CNN.mat'\n",
    "LabelFile='Label_OneHot.mat'\n",
    "#print(\"Loading data and labels...\")\n",
    "Data=load_mat(Path+DataFile,'Data')\n",
    "Label=load_mat(Path+LabelFile,'Label')\n",
    "Data=Data.T\n",
    "Indices=np.arange(Data.shape[0]) #随机打乱索引并切分训练集与测试集\n",
    "np.random.shuffle(Indices)\n",
    "\n",
    "#print(\"Divide training and testing set...\")\n",
    "train_x=Data[Indices[:5000]]\n",
    "train_y=Label[Indices[:5000]]\n",
    "test_x=Data[Indices[15000:]]\n",
    "test_y=Label[Indices[15000:]]\n",
    "\n",
    "train_y[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_vectors=[]\n",
    "for i in range(0,len(train_x)):\n",
    "    list(train_x[i])\n",
    "    hot_vectors.append(train_x[i])\n",
    "\n",
    "labels=[];\n",
    "for i in range(0,len(train_y)):\n",
    "    list(train_y[i])\n",
    "    labels.append(train_y[i])\n",
    "#labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from visualdl import LogWriter\n",
    "#with LogWriter(logdir=\"./log/high_dimensional_test/train\") as writer:\n",
    "        # 将一组labels和对应的hot_vectors传入记录器进行记录\n",
    "#        writer.add_embeddings(tag='default',\n",
    "#                              labels=labels,\n",
    "#                              hot_vectors=hot_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_data(data.Dataset):\n",
    "    def __init__(self, x, y,train, transform=None):\n",
    "        self.xsignals =torch.from_numpy(x)\n",
    "        self.ylabels = torch.from_numpy(y).max(dim=1)[1]\n",
    "        self.train=train\n",
    "        if transform is not None:   \n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self.transform = None\n",
    "\n",
    "    def __getitem__(self, index):   #必须加载的方法\n",
    "        signal = self.xsignals[index]\n",
    "        Label = self.ylabels[index]\n",
    "        return signal, Label   #返回处理完的图片数据和标签\n",
    "\n",
    "    def __len__(self):     #必须加载的方法,实际上好像没什么用\n",
    "        return len(self.ylabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader=torch.utils.data.DataLoader(dataset=my_data(train_x,train_y,1))\n",
    "test_dataloader=torch.utils.data.DataLoader(dataset=my_data(test_x,test_y,0))\n",
    "train_dataset=my_data(train_x,train_y,1)\n",
    "test_dataset=my_data(test_x,test_y,0)\n",
    "train_dataset.ylabels.shape\n",
    "train_dataset.ylabels.shape\n",
    "print(train_dataset.ylabels[0])\n",
    "#torch.argmax(train_dataset.ylabels[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class SiameseECG(Dataset):\n",
    "    \"\"\"\n",
    "    Train: For each sample creates randomly a positive or a negative pair\n",
    "    Test: Creates fixed pairs for testing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ecg_dataset):\n",
    "        self.ecg_dataset = ecg_dataset\n",
    "\n",
    "        self.train = self.ecg_dataset.train\n",
    "        self.transform = self.ecg_dataset.transform\n",
    "\n",
    "        if self.train:\n",
    "            self.train_labels = self.ecg_dataset.ylabels\n",
    "            self.train_data = self.ecg_dataset.xsignals\n",
    "            self.labels_set = set(self.train_labels)\n",
    "            self.label_to_indices = {label: np.where(self.train_labels == label)[0]\n",
    "                                     for label in self.labels_set}\n",
    "        else:\n",
    "            # generate fixed pairs for testing\n",
    "            self.test_labels = self.ecg_dataset.ylabels\n",
    "            self.test_data = self.ecg_dataset.xsignals\n",
    "            self.labels_set = list(set(self.test_labels))\n",
    "            self.label_to_indices = {label: np.where(self.test_labels.numpy() == label)[0]\n",
    "                                     for label in self.labels_set}\n",
    "            print(self.test_labels)\n",
    "            print(self.label_to_indices)\n",
    "            print(self.label_to_indices[self.test_labels])\n",
    "            random_state = np.random.RandomState(29)\n",
    "\n",
    "            positive_pairs = [[i,\n",
    "                               random_state.choice(self.label_to_indices[self.test_labels[i].item()]),\n",
    "                               1]\n",
    "                              for i in range(0, len(self.test_data), 2)]\n",
    "\n",
    "            negative_pairs = [[i,\n",
    "                               random_state.choice(self.label_to_indices[\n",
    "                                                       np.random.choice(\n",
    "                                                           list(self.labels_set - set([self.test_labels[i]]))\n",
    "                                                       )\n",
    "                                                   ]),\n",
    "                               0]\n",
    "                              for i in range(1, len(self.test_data), 2)]\n",
    "            self.test_pairs = positive_pairs + negative_pairs\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            target = np.random.randint(0, 2)\n",
    "            img1, label1 = self.train_data[index], self.train_labels[index].item()\n",
    "            if target == 1:\n",
    "                siamese_index = index\n",
    "                while siamese_index == index:\n",
    "                    siamese_index = np.random.choice(self.label_to_indices[label1])\n",
    "            else:\n",
    "                siamese_label = np.random.choice(list(self.labels_set - set([label1])))\n",
    "                siamese_index = np.random.choice(self.label_to_indices[siamese_label])\n",
    "            img2 = self.train_data[siamese_index]\n",
    "        else:\n",
    "            img1 = self.test_data[self.test_pairs[index][0]]\n",
    "            img2 = self.test_data[self.test_pairs[index][1]]\n",
    "            target = self.test_pairs[index][2]\n",
    "\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "        return (img1, img2), target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ecg_dataset)\n",
    "\n",
    "siamese_train_dataset = SiameseECG(train_dataset) # Returns pairs of images and target same/different\n",
    "#siamese_test_dataset = SiameseECG(test_dataset)\n",
    "batch_size = 128\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "siamese_train_loader = torch.utils.data.DataLoader(siamese_train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "#siamese_test_loader = torch.utils.data.DataLoader(siamese_test_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "# Set up the network and training parameters\n",
    "from networks import EmbeddingNet, SiameseNet\n",
    "from loss import ContrastiveLoss\n",
    "\n",
    "margin = 1.\n",
    "embedding_net = EmbeddingNet()\n",
    "model = SiameseNet(embedding_net)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "loss_fn = ContrastiveLoss(margin)\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 20\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from visualdl import LogWriter\n",
    "with LogWriter(logdir=\"./log/scalar_test/train\") as writer:\n",
    "    for t in range(epochs):\n",
    "        for step, (x, y) in enumerate(siamese_train_loader):\n",
    "            # Forward pass: Compute predicted y by passing x to the model\n",
    "            y_pred = model(x.cuda())\n",
    "\n",
    "            # Compute and print loss\n",
    "            loss = loss_fn(y_pred, y.cuda()) # 计算损失函数\n",
    "\n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            optimizer.zero_grad() # 梯度置零，因为反向传播过程中梯度会累加上一次循环的梯度\n",
    "            loss.backward() # loss反向传播\n",
    "            optimizer.step() # 反向传播后参数更新\n",
    "            if(step%log_interval==0):\n",
    "                print(loss)\n",
    "\n",
    "            writer.add_scalar(tag=\"loss\",step=step,value=loss)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c51de5c7900c6feca581576e9f5905dbfe728184fc376a8e1a7553afc0ae3498"
  },
  "kernelspec": {
   "display_name": "PyCharm (ECG-ML-DL-Algorithm-Python-master)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}